{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import argparse\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "from qtransformer import TextClassifier\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = torch.LongTensor(batch.text[0])\n",
    "        if inputs.size(1) > MAX_SEQ_LEN:\n",
    "            inputs = inputs[:, :MAX_SEQ_LEN]\n",
    "        predictions = model(inputs).squeeze(1)\n",
    "        \n",
    "        label = batch.label - 1\n",
    "        #label = label.unsqueeze(1)\n",
    "        loss = criterion(predictions, label)\n",
    "        #loss = F.nll_loss(predictions, label)\n",
    "        acc = binary_accuracy(predictions, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            inputs = torch.LongTensor(batch.text[0])\n",
    "            if inputs.size(1) > MAX_SEQ_LEN:\n",
    "                inputs = inputs[:, :MAX_SEQ_LEN]\n",
    "            predictions = model(inputs).squeeze(1)\n",
    "            \n",
    "            label = batch.label - 1\n",
    "            #label = label.unsqueeze(1)\n",
    "            loss = criterion(predictions, label)\n",
    "            #loss = F.nll_loss(predictions, label)\n",
    "            acc = binary_accuracy(predictions, label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-D', '--q_device', default='local', type=str)\n",
    "    parser.add_argument('-B', '--batch_size', default=32, type=int)\n",
    "    parser.add_argument('-E', '--n_epochs', default=5, type=int)\n",
    "    parser.add_argument('-C', '--n_classes', default=2, type=int)\n",
    "    parser.add_argument('-l', '--lr', default=0.001, type=float)\n",
    "    parser.add_argument('-v', '--vocab_size', default=20000, type=int)\n",
    "    parser.add_argument('-e', '--embed_dim', default=8, type=int)\n",
    "    parser.add_argument('-s', '--max_seq_len', default=64, type=int)\n",
    "    parser.add_argument('-f', '--ffn_dim', default=8, type=int)\n",
    "    parser.add_argument('-t', '--n_transformer_blocks', default=1, type=int)\n",
    "    parser.add_argument('-H', '--n_heads', default=2, type=int)\n",
    "    parser.add_argument('-q', '--n_qubits_transformer', default=0, type=int)\n",
    "    parser.add_argument('-Q', '--n_qubits_ffn', default=0, type=int)\n",
    "    parser.add_argument('-L', '--n_qlayers', default=1, type=int)\n",
    "    parser.add_argument('-d', '--dropout_rate', default=0.1, type=float)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    MAX_SEQ_LEN = args.max_seq_len\n",
    "\n",
    "    TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "    #LABEL = data.Field(sequential=False)\n",
    "    LABEL = data.LabelField(dtype=torch.float)\n",
    "    train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
    "    print(f'Training examples: {len(train_data)}')\n",
    "    print(f'Testing examples:  {len(test_data)}')\n",
    "\n",
    "    TEXT.build_vocab(train_data, max_size=args.vocab_size - 2)  # exclude <UNK> and <PAD>\n",
    "    LABEL.build_vocab(train_data)\n",
    "\n",
    "    train_iter, test_iter = data.BucketIterator.splits((train_data, test_data), batch_size=args.batch_size)\n",
    "    \n",
    "    model = TextClassifier(embed_dim=args.embed_dim,\n",
    "                           num_heads=args.n_heads,\n",
    "                           num_blocks=args.n_transformer_blocks,\n",
    "                           num_classes=args.n_classes,\n",
    "                           vocab_size=args.vocab_size,\n",
    "                           ffn_dim=args.ffn_dim,\n",
    "                           n_qubits_transformer=args.n_qubits_transformer,\n",
    "                           n_qubits_ffn=args.n_qubits_ffn,\n",
    "                           n_qlayers=args.n_qlayers,\n",
    "                           dropout=args.dropout_rate,\n",
    "                           q_device=args.q_device)\n",
    "    print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "    optimizer = torch.optim.Adam(lr=args.lr, params=model.parameters())\n",
    "    if args.n_classes < 3:\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()  # logits -> sigmoid -> loss\n",
    "    else:\n",
    "        criterion = torch.nn.CrossEntropyLoss()  # logits -> log_softmax -> NLLloss\n",
    "\n",
    "    # training loop\n",
    "    best_valid_loss = float('inf')\n",
    "    for iepoch in range(args.n_epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(f\"Epoch {iepoch+1}/{args.n_epochs}\")\n",
    "\n",
    "        train_loss, train_acc = train(model, train_iter, optimizer, criterion)\n",
    "        valid_loss, valid_acc = evaluate(model, test_iter, criterion)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'model.pt')\n",
    "        \n",
    "        print(f'Epoch: {iepoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Qistik')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "667df7373ffb018a1a239c470215b88f7a3f7ff7ba7f62fed15530f4f71cc92c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
