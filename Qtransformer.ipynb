{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fd81fbb-4592-4dbe-a67c-d194782fc411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from qiskit import QuantumCircuit, transpile, Aer, assemble\n",
    "import pennylane as qml\n",
    "\n",
    "class MultiHeadAttentionBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionBase, self).__init__()\n",
    "\n",
    "        assert embed_dim % num_heads == 0, f\"Embedding dimension ({embed_dim}) should be divisible by number of heads ({num_heads})\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = embed_dim // num_heads  # projection dimensions\n",
    "        self.k_linear = None\n",
    "        self.q_linear = None\n",
    "        self.v_linear = None\n",
    "        self.combine_heads = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn_weights = None\n",
    "    \n",
    "    def separate_heads(self, x):\n",
    "        '''\n",
    "        split into N heads\n",
    "        from (batch_size, seq_len, embed_dim)\n",
    "        to   (batch_size, seq_len, num_heads, embed_dim)\n",
    "        then transpose (1,2) to (batch_size, num_heads, seq_len, embed_dim)\n",
    "        to make mat mult straightforward for each head\n",
    "        '''\n",
    "        batch_size = x.size(0)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 2)\n",
    "\n",
    "    def attention(self, query, key, value, mask=None, dropout=None):\n",
    "        '''\n",
    "        Attention(Q, K, V) = softmax(Q K^T / sqrt(d_k))V\n",
    "        '''\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        # see also: https://tensorchiefs.github.io/dlday2018/tutorial/einsum.html\n",
    "        #scores = torch.einsum('bijh, bkjh -> bikh', query, key) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if dropout is not None:\n",
    "            scores = dropout(scores)\n",
    "        attn = torch.matmul(scores, value)\n",
    "        return attn, scores\n",
    "    \n",
    "    def downstream(self, query, key, value, batch_size, mask=None):\n",
    "        Q = self.separate_heads(query)\n",
    "        K = self.separate_heads(key)\n",
    "        V = self.separate_heads(value)\n",
    "\n",
    "        x, self.attn_weights = self.attention(Q, K, V, mask, dropout=self.dropout)\n",
    "\n",
    "        concat = x.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
    "\n",
    "        return concat\n",
    "        # output = self.combine_heads(concat)\n",
    "        # return output\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        raise NotImplementedError(\"Base class does not execute forward function.\")\n",
    "\n",
    "class MultiHeadAttentionClassical(MultiHeadAttentionBase):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False):\n",
    "        super(MultiHeadAttentionClassical, self).__init__(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "\n",
    "        self.k_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.q_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.v_linear = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.combine_heads = nn.Linear(embed_dim, embed_dim, bias=use_bias)\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = self.k_linear(x)\n",
    "        Q = self.q_linear(x)\n",
    "        V = self.v_linear(x)\n",
    "\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        output = self.combine_heads(x)\n",
    "        return output\n",
    "\n",
    "class MultiHeadAttentionQuantum(MultiHeadAttentionBase):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 dropout=0.1,\n",
    "                 mask=None,\n",
    "                 use_bias=False,\n",
    "                 n_qubits: int = 16,\n",
    "                 n_qlayers: int = 1,\n",
    "                 q_device=\"default.qubit\"):\n",
    "        super(MultiHeadAttentionQuantum, self).__init__(embed_dim, num_heads, dropout=dropout, mask=mask, use_bias=use_bias)\n",
    "        \n",
    "        # todo: add intermediate layer to \"dress\" quantum circuit\n",
    "        assert n_qubits == embed_dim, \"Number of qubits ({n_qubits}) does not match embedding dim ({embed_dim})\"\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.q_device = q_device\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        if 'qulacs' in q_device:\n",
    "            self.dev = qml.device(q_device, wires=self.n_qubits, gpu=True)\n",
    "        elif 'braket' in q_device:\n",
    "            self.dev = qml.device(q_device, wires=self.n_qubits, parallel=True)\n",
    "        else:\n",
    "            self.dev = qml.device(q_device, wires=self.n_qubits)\n",
    "\n",
    "        def _circuit(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=range(self.n_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "\n",
    "        self.qlayer = qml.QNode(_circuit, self.dev, interface=\"torch\")\n",
    "        self.weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_qubits) = ({n_qlayers}, {self.n_qubits})\")\n",
    "\n",
    "        self.k_linear = qml.qnn.TorchLayer(self.qlayer, self.weight_shapes)\n",
    "        self.q_linear = qml.qnn.TorchLayer(self.qlayer, self.weight_shapes)\n",
    "        self.v_linear = qml.qnn.TorchLayer(self.qlayer, self.weight_shapes)\n",
    "        self.combine_heads = qml.qnn.TorchLayer(self.qlayer, self.weight_shapes)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        assert embed_dim == self.embed_dim, f\"Input embedding ({embed_dim}) does not match layer embedding size ({self.embed_dim})\"\n",
    "\n",
    "        K = [self.k_linear(x[:, t, :]) for t in range(seq_len)]\n",
    "        Q = [self.q_linear(x[:, t, :]) for t in range(seq_len)]\n",
    "        V = [self.v_linear(x[:, t, :]) for t in range(seq_len)]\n",
    "\n",
    "        K = torch.Tensor(pad_sequence(K))\n",
    "        Q = torch.Tensor(pad_sequence(Q))\n",
    "        V = torch.Tensor(pad_sequence(V))\n",
    "\n",
    "        x = self.downstream(Q, K, V, batch_size, mask)\n",
    "        output = [self.combine_heads(x[:, t, :]) for t in range(seq_len)]\n",
    "        output = torch.Tensor(pad_sequence(output))\n",
    "        return output\n",
    "\n",
    "class FeedForwardBase(nn.Module):\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.1):\n",
    "        super(FeedForwardBase, self).__init__()\n",
    "        self.linear_1 = nn.Linear(embed_dim, ffn_dim)\n",
    "        self.linear_2 = nn.Linear(ffn_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError(\"Base class does not implement forward function\")\n",
    "\n",
    "\n",
    "class FeedForwardClassical(FeedForwardBase):\n",
    "    def __init__(self, embed_dim, ffn_dim, dropout=0.1):\n",
    "        super(FeedForwardClassical, self).__init__(embed_dim, ffn_dim, dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForwardQuantum(FeedForwardBase):\n",
    "    def __init__(self, embed_dim, n_qubits, n_qlayers=1, dropout=0.1, q_device=\"default.qubit\"):\n",
    "        super(FeedForwardQuantum, self).__init__(embed_dim, ffn_dim=n_qubits, dropout=dropout)\n",
    "\n",
    "        self.n_qubits = n_qubits\n",
    "        if 'qulacs' in q_device:\n",
    "            self.dev = qml.device(q_device, wires=self.n_qubits, gpu=True)\n",
    "        elif 'braket' in q_device:\n",
    "            self.dev = qml.device(q_device, wires=self.n_qubits, parallel=True)\n",
    "        else:\n",
    "            self.dev = qml.device(q_device, wires=self.n_qubits)\n",
    "\n",
    "        def _circuit(inputs, weights):\n",
    "            qml.templates.AngleEmbedding(inputs, wires=range(self.n_qubits))\n",
    "            qml.templates.BasicEntanglerLayers(weights, wires=range(n_qubits))\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in range(n_qubits)]\n",
    "        self.qlayer = qml.QNode(_circuit, self.dev, interface=\"torch\")\n",
    "        self.weight_shapes = {\"weights\": (n_qlayers, n_qubits)}\n",
    "        self.vqc = qml.qnn.TorchLayer(self.qlayer, self.weight_shapes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        x = self.linear_1(x)\n",
    "        X = [self.vqc(x[:, t, :]) for t in range(seq_len)]\n",
    "        x = torch.Tensor(pad_sequence(X))\n",
    "        # dropout?\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlockBase(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_head: int,\n",
    "                 ff_dim: int,\n",
    "                 n_qubits_transformer: int = 0,\n",
    "                 n_qubits_ffn: int = 0,\n",
    "                 n_qlayers: int = 1,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None):\n",
    "        super(TransformerBlockBase, self).__init__()\n",
    "        self.attn = None\n",
    "        self.ffn = None\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output = self.attn(x)\n",
    "        x = self.norm1(attn_output + x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        ff_output = self.ffn(x)\n",
    "        x = self.norm2(ff_output + x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlockClassical(TransformerBlockBase):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 ff_dim: int,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None):\n",
    "        super(TransformerBlockClassical, self).__init__(embed_dim, num_heads, ff_dim, dropout, mask)\n",
    "        self.attn = MultiHeadAttentionClassical(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, mask=mask)\n",
    "        self.ffn = FeedForwardClassical(embed_dim, ff_dim)\n",
    "\n",
    "\n",
    "class TransformerBlockQuantum(TransformerBlockBase):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 ffn_dim: int,\n",
    "                 n_qubits_transformer: int = 0,\n",
    "                 n_qubits_ffn: int = 0,\n",
    "                 n_qlayers: int = 1,\n",
    "                 dropout: float = 0.1,\n",
    "                 mask=None,\n",
    "                 q_device='default.qubit'):\n",
    "        super(TransformerBlockQuantum, self).__init__(embed_dim, num_heads, ffn_dim, dropout, mask)\n",
    "        \n",
    "        self.n_qubits_transformer = n_qubits_transformer\n",
    "        self.n_qubits_ffn = n_qubits_ffn\n",
    "        self.n_qlayers = n_qlayers\n",
    "\n",
    "        self.attn = MultiHeadAttentionQuantum(embed_dim,\n",
    "                                              num_heads,\n",
    "                                              n_qubits=n_qubits_transformer,\n",
    "                                              n_qlayers=n_qlayers,\n",
    "                                              dropout=dropout,\n",
    "                                              mask=mask,\n",
    "                                              q_device=q_device)\n",
    "        if n_qubits_ffn > 0:\n",
    "            self.ffn = FeedForwardQuantum(embed_dim, n_qubits_ffn, n_qlayers, q_device=q_device)\n",
    "        else:\n",
    "            self.ffn = FeedForwardClassical(embed_dim, ffn_dim)\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_le\n",
    "            transformer_blocks = [\n",
    "                TransformerBlockClassical(embed_dim, num_heads, ffn_dim) for _ in range(num_blocks)\n",
    "            ]\n",
    "\n",
    "        self.transformers = nn.Sequential(*transformer_blocks)\n",
    "        if self.num_classes > 2:\n",
    "            self.class_logits = nn.Linear(embed_dim, num_classes)\n",
    "        else:\n",
    "            self.class_logits = nn.Linear(embed_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.token_embedding(x)\n",
    "        # batch_size, seq_len, embed_dim = x.size()\n",
    "        x = self.pos_embedding(tokens)\n",
    "        x = self.transformers(x)\n",
    "        x = x.mean(dim=1)  # global average pooling, works in 1D\n",
    "        x = self.dropout(x)\n",
    "        # x = self.class_logits(x)\n",
    "        # return F.log_softmax(x, dim=1)\n",
    "        return self.class_logits(x)n=512):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on pos and i\n",
    "        pe = torch.zeros(max_seq_len, embed_dim)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, embed_dim, 2):\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embed_dim)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/embed_dim)))\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.embed_dim)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + torch.autograd.Variable(self.pe[:,:seq_len], requires_grad=False)  # .cuda()\n",
    "        return x\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embed_dim: int,\n",
    "                 num_heads: int,\n",
    "                 num_blocks: int,\n",
    "                 num_classes: int,\n",
    "                 vocab_size: int,\n",
    "                 ffn_dim: int = 32,\n",
    "                 n_qubits_transformer: int = 0,\n",
    "                 n_qubits_ffn: int = 0,\n",
    "                 n_qlayers: int = 1,\n",
    "                 dropout=0.1,\n",
    "                 q_device=\"device.qubit\"):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_blocks = num_blocks\n",
    "        self.num_classes = num_classes\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embedding = PositionalEncoder(embed_dim)\n",
    "\n",
    "        print(f\"++ There will be {num_blocks} transformer blocks\")\n",
    "\n",
    "        if n_qubits_transformer > 0:\n",
    "            print(f\"++ Transformer will use {n_qubits_transformer} qubits and {n_qlayers} q layers\")\n",
    "            if n_qubits_ffn > 0:\n",
    "                print(f\"The feed-forward head will use {n_qubits_ffn} qubits\")\n",
    "            else:\n",
    "                print(f\"The feed-forward head will be classical\")\n",
    "            print(f\"Using quantum device {q_device}\")\n",
    "\n",
    "            transformer_blocks = [\n",
    "                TransformerBlockQuantum(embed_dim, num_heads, ffn_dim,\n",
    "                                        n_qubits_transformer=n_qubits_transformer,\n",
    "                                        n_qubits_ffn=n_qubits_ffn,\n",
    "                                        n_qlayers=n_qlayers, \n",
    "                                        q_device=q_device) for _ in range(num_blocks)\n",
    "            ]\n",
    "        else:\n",
    "            transformer_blocks = [\n",
    "                TransformerBlockClassical(embed_dim, num_heads, ffn_dim) for _ in range(num_blocks)\n",
    "            ]\n",
    "\n",
    "        self.transformers = nn.Sequential(*transformer_blocks)\n",
    "        if self.num_classes > 2:\n",
    "            self.class_logits = nn.Linear(embed_dim, num_classes)\n",
    "        else:\n",
    "            self.class_logits = nn.Linear(embed_dim, 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tokens = self.token_embedding(x)\n",
    "        # batch_size, seq_len, embed_dim = x.size()\n",
    "        x = self.pos_embedding(tokens)\n",
    "        x = self.transformers(x)\n",
    "        x = x.mean(dim=1)  # global average pooling, works in 1D\n",
    "        x = self.dropout(x)\n",
    "        # x = self.class_logits(x)\n",
    "        # return F.log_softmax(x, dim=1)\n",
    "        return self.class_logits(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6b1cd56-36ef-4f6b-8a2d-a6ee0852ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_shapes = (n_qlayers, n_qubits) = (1, 16)\n",
      "out_q:\n",
      "tensor([[[ 9.1934e-04,  1.3679e-02,  1.3462e-02, -5.9553e-03, -5.1082e-03,\n",
      "          -3.7311e-03,  2.2146e-03, -6.2246e-04,  3.3670e-04, -3.3611e-04,\n",
      "          -2.6084e-04, -1.6018e-04,  1.5706e-04,  8.5215e-05, -4.8265e-05,\n",
      "           2.2669e-05],\n",
      "         [ 1.2854e-03,  1.6908e-02,  1.6113e-02, -8.3684e-03, -7.2838e-03,\n",
      "          -5.1987e-03,  3.0728e-03, -8.7038e-04,  4.7081e-04, -4.6999e-04,\n",
      "          -3.6473e-04, -2.2398e-04,  2.1961e-04,  1.1916e-04, -6.7489e-05,\n",
      "           3.1698e-05],\n",
      "         [ 1.1288e-03,  1.6270e-02,  1.5852e-02, -7.3465e-03, -6.3927e-03,\n",
      "          -4.5648e-03,  2.6979e-03, -7.6428e-04,  4.1341e-04, -4.1269e-04,\n",
      "          -3.2026e-04, -1.9667e-04,  1.9284e-04,  1.0463e-04, -5.9261e-05,\n",
      "           2.7833e-05],\n",
      "         [ 1.5351e-03,  1.9078e-02,  1.7828e-02, -9.9927e-03, -8.6974e-03,\n",
      "          -6.2079e-03,  3.6693e-03, -1.0393e-03,  5.6222e-04, -5.6124e-04,\n",
      "          -4.3554e-04, -2.6746e-04,  2.6225e-04,  1.4229e-04, -8.0592e-05,\n",
      "           3.7852e-05]],\n",
      "\n",
      "        [[ 1.7566e-03,  2.1732e-02,  1.5211e-02, -1.3437e-02, -1.2822e-02,\n",
      "          -7.8714e-03,  4.7069e-03, -1.1258e-03,  6.3365e-04, -6.3310e-04,\n",
      "          -4.8993e-04, -3.0194e-04,  2.9613e-04,  1.6034e-04, -9.0656e-05,\n",
      "           4.2550e-05],\n",
      "         [ 1.2896e-03,  2.1731e-02,  1.5214e-02, -1.3439e-02, -1.3171e-02,\n",
      "          -7.0135e-03,  3.6295e-03, -8.6700e-04,  4.6616e-04, -4.6543e-04,\n",
      "          -3.6120e-04, -2.2169e-04,  2.1742e-04,  1.1772e-04, -6.6560e-05,\n",
      "           3.1240e-05],\n",
      "         [ 1.3729e-03,  2.1730e-02,  1.5210e-02, -1.3435e-02, -1.3168e-02,\n",
      "          -7.0111e-03,  3.6329e-03, -8.6705e-04,  4.9512e-04, -4.9472e-04,\n",
      "          -3.8268e-04, -2.3600e-04,  2.3146e-04,  1.2531e-04, -7.0850e-05,\n",
      "           3.3253e-05],\n",
      "         [ 1.3523e-03,  2.1729e-02,  1.5210e-02, -1.3435e-02, -1.3167e-02,\n",
      "          -7.0108e-03,  3.6332e-03, -8.6711e-04,  4.8773e-04, -4.8731e-04,\n",
      "          -3.7709e-04, -2.3242e-04,  2.2795e-04,  1.2342e-04, -6.9783e-05,\n",
      "           3.2752e-05]],\n",
      "\n",
      "        [[ 4.4036e-04,  8.8126e-03,  8.8122e-03, -2.7451e-03, -1.9861e-03,\n",
      "          -1.7074e-03,  1.0892e-03, -2.9567e-04,  1.6218e-04, -1.6189e-04,\n",
      "          -1.2572e-04, -7.7140e-05,  7.5638e-05,  4.1030e-05, -2.3240e-05,\n",
      "           1.0916e-05],\n",
      "         [ 4.2781e-04,  8.6316e-03,  8.6307e-03, -2.6529e-03, -1.9094e-03,\n",
      "          -1.6444e-03,  1.0523e-03, -2.8709e-04,  1.5747e-04, -1.5719e-04,\n",
      "          -1.2207e-04, -7.4901e-05,  7.3443e-05,  3.9839e-05, -2.2565e-05,\n",
      "           1.0599e-05],\n",
      "         [ 4.6315e-04,  9.0559e-03,  9.0559e-03, -2.8759e-03, -2.0702e-03,\n",
      "          -1.7827e-03,  1.1406e-03, -3.1118e-04,  1.7069e-04, -1.7038e-04,\n",
      "          -1.3231e-04, -8.1186e-05,  7.9606e-05,  4.3182e-05, -2.4459e-05,\n",
      "           1.1488e-05],\n",
      "         [ 2.9542e-04,  6.2486e-03,  6.2442e-03, -1.8165e-03, -1.3073e-03,\n",
      "          -1.1259e-03,  7.2053e-04, -1.9658e-04,  1.0788e-04, -1.0769e-04,\n",
      "          -8.3614e-05, -5.1319e-05,  5.0320e-05,  2.7296e-05, -1.5461e-05,\n",
      "           7.2620e-06]],\n",
      "\n",
      "        [[ 1.4745e-03,  1.9726e-02,  1.8625e-02, -8.5197e-03, -7.1981e-03,\n",
      "          -5.5925e-03,  2.9513e-03, -1.0167e-03,  5.1986e-04, -5.1838e-04,\n",
      "          -4.0485e-04, -2.4623e-04,  2.4119e-04,  1.3160e-04, -7.4286e-05,\n",
      "           3.4741e-05],\n",
      "         [ 1.4379e-03,  1.8578e-02,  1.7592e-02, -7.8284e-03, -6.4970e-03,\n",
      "          -5.1521e-03,  2.8313e-03, -9.1468e-04,  4.7205e-04, -4.7077e-04,\n",
      "          -3.6726e-04, -2.2376e-04,  2.1928e-04,  1.1923e-04, -6.7450e-05,\n",
      "           3.1630e-05],\n",
      "         [ 1.0528e-03,  1.6459e-02,  1.6206e-02, -6.1467e-03, -5.1919e-03,\n",
      "          -4.0347e-03,  2.1302e-03, -7.3345e-04,  3.7503e-04, -3.7396e-04,\n",
      "          -2.9206e-04, -1.7763e-04,  1.7399e-04,  9.4935e-05, -5.3590e-05,\n",
      "           2.5062e-05],\n",
      "         [ 1.7702e-03,  2.0545e-02,  1.8152e-02, -9.4423e-03, -7.9878e-03,\n",
      "          -6.1985e-03,  3.2638e-03, -1.1273e-03,  5.7640e-04, -5.7475e-04,\n",
      "          -4.4888e-04, -2.7301e-04,  2.6742e-04,  1.4591e-04, -8.2366e-05,\n",
      "           3.8519e-05]]], grad_fn=<AliasBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "embed_size = 16\n",
    "heads = 4\n",
    "value_len = 4\n",
    "key_len = 4\n",
    "query_len = 4\n",
    "N = 4\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Instantiate the QuantumSelfAttention module\n",
    "qsa_Quantum = MultiHeadAttentionQuantum(embed_size, heads)\n",
    "qsa_Classical = MultiHeadAttentionClassical(embed_size, heads)\n",
    "# Generate random values, keys, queries, and mask\n",
    "values = torch.randn(N, value_len, heads, qsa_Quantum.head_dim)\n",
    "keys = torch.randn(N, key_len, heads, qsa_Quantum.head_dim)\n",
    "queries = torch.randn(N, query_len, heads, qsa_Quantum.head_dim)\n",
    "mask = torch.ones(N, query_len, key_len)\n",
    "x_q = qsa_Quantum.downstream(queries, keys, values, N, mask)\n",
    "out_q = qsa_Quantum(x_q, mask)\n",
    "\n",
    "print(\"out_q:\")\n",
    "print(out_q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea0ec730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_c:\n",
      "tensor([[[-0.1598,  0.0918,  0.1000,  0.0930, -0.1281,  0.0658, -0.1756,\n",
      "           0.0853,  0.0256, -0.0069, -0.2796, -0.2439, -0.1438,  0.0064,\n",
      "           0.0707,  0.2677],\n",
      "         [-0.1972,  0.1191,  0.1033,  0.0780, -0.1503,  0.0826, -0.1941,\n",
      "           0.1044,  0.0169, -0.0244, -0.2912, -0.2563, -0.1859, -0.0179,\n",
      "           0.0327,  0.3085],\n",
      "         [-0.2243,  0.1408,  0.1115,  0.0802, -0.1532,  0.0667, -0.1783,\n",
      "           0.1599,  0.0127, -0.0547, -0.3139, -0.2527, -0.2357, -0.0086,\n",
      "           0.0213,  0.2924],\n",
      "         [-0.2390,  0.1785,  0.1053,  0.0630, -0.1724,  0.0450, -0.1905,\n",
      "           0.1624,  0.0283, -0.0786, -0.3114, -0.2471, -0.2167,  0.0135,\n",
      "           0.0033,  0.2659]],\n",
      "\n",
      "        [[-0.3112, -0.0482,  0.0032, -0.0293,  0.1031,  0.0432, -0.0412,\n",
      "           0.0608, -0.0408, -0.2106,  0.0133, -0.0696,  0.0805, -0.1356,\n",
      "          -0.0376, -0.0929],\n",
      "         [-0.4017,  0.0316, -0.0315, -0.0845,  0.1029,  0.0494,  0.0059,\n",
      "           0.1573, -0.0324, -0.3441,  0.0415,  0.0080,  0.1007, -0.1400,\n",
      "          -0.0024, -0.1764],\n",
      "         [-0.3984,  0.0109, -0.0339, -0.0727,  0.1068,  0.0521,  0.0163,\n",
      "           0.1582, -0.0377, -0.3333,  0.0331,  0.0135,  0.0842, -0.1473,\n",
      "           0.0053, -0.1686],\n",
      "         [-0.3990,  0.0114, -0.0336, -0.0731,  0.1066,  0.0526,  0.0158,\n",
      "           0.1569, -0.0377, -0.3329,  0.0335,  0.0132,  0.0854, -0.1479,\n",
      "           0.0053, -0.1683]],\n",
      "\n",
      "        [[-0.1391,  0.0245,  0.0448,  0.0732, -0.1862, -0.1391, -0.0564,\n",
      "           0.0694, -0.0056, -0.0750, -0.2478, -0.0568, -0.2150,  0.0167,\n",
      "           0.0474,  0.0675],\n",
      "         [-0.1351,  0.1402,  0.0314,  0.0197, -0.0878, -0.1182, -0.0646,\n",
      "           0.0622,  0.0379, -0.0486, -0.1850,  0.0153, -0.1429,  0.0540,\n",
      "          -0.0690, -0.0861],\n",
      "         [-0.1516,  0.0069,  0.0572,  0.0851, -0.2065, -0.1428, -0.1097,\n",
      "           0.0106, -0.0145, -0.0354, -0.2628, -0.0750, -0.2327,  0.0060,\n",
      "           0.1363,  0.1317],\n",
      "         [-0.1319,  0.0319,  0.0694,  0.0637, -0.1682, -0.1440, -0.1095,\n",
      "           0.0264,  0.0050, -0.0376, -0.2463, -0.0690, -0.2078,  0.0149,\n",
      "           0.0852,  0.0730]],\n",
      "\n",
      "        [[ 0.1687,  0.0667,  0.2433, -0.0299, -0.0203, -0.2879, -0.0563,\n",
      "          -0.0901,  0.0644,  0.1127,  0.1543,  0.0425,  0.0943,  0.2724,\n",
      "          -0.1957, -0.2121],\n",
      "         [ 0.1798,  0.0492,  0.1565, -0.0338, -0.0294, -0.2117, -0.0504,\n",
      "          -0.0514,  0.0784,  0.0407,  0.1934, -0.0130,  0.0748,  0.1873,\n",
      "          -0.2129, -0.1474],\n",
      "         [ 0.1882,  0.0798,  0.2338, -0.0584, -0.0760, -0.2820, -0.0613,\n",
      "          -0.0559,  0.0610,  0.0808,  0.1235, -0.0015,  0.0812,  0.2727,\n",
      "          -0.2122, -0.1409],\n",
      "         [ 0.1747,  0.0675,  0.2156, -0.0484, -0.1122, -0.3038,  0.0129,\n",
      "          -0.0689,  0.0567,  0.0775,  0.1574,  0.0902,  0.0812,  0.2622,\n",
      "          -0.2186, -0.1856]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Generate random values, keys, queries, and mask\n",
    "values_c = torch.randn(N, value_len, heads, qsa_Classical.head_dim)\n",
    "keys_c = torch.randn(N, key_len, heads, qsa_Classical.head_dim)\n",
    "queries_c = torch.randn(N, query_len, heads, qsa_Classical.head_dim)\n",
    "#mask = torch.ones(N, query_len, key_len)\n",
    "x_c = qsa_Classical.downstream(queries, keys, values, N, mask)\n",
    "out_c = qsa_Classical(x_c, mask)\n",
    "\n",
    "print(\"out_c:\")\n",
    "print(out_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "349fab16-9ea6-4734-aa87-c3f99534a269",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"word-wrap: normal;white-space: pre;background: #fff0;line-height: 1.1;font-family: &quot;Courier New&quot;,Courier,monospace\">        ┌─────────┐┌─────────┐┌─────────┐ ░ ┌─┐         \n",
       "   q_0: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░─┤M├─────────\n",
       "        ├─────────┤├─────────┤├─────────┤ ░ └╥┘┌─┐      \n",
       "   q_1: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░──╫─┤M├──────\n",
       "        ├─────────┤├─────────┤├─────────┤ ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░──╫──╫─┤M├───\n",
       "        ├─────────┤├─────────┤├─────────┤ ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░──╫──╫──╫─┤M├\n",
       "        └─────────┘└─────────┘└─────────┘ ░  ║  ║  ║ └╥┘\n",
       "meas: 4/═════════════════════════════════════╩══╩══╩══╩═\n",
       "                                             0  1  2  3 </pre>"
      ],
      "text/plain": [
       "        ┌─────────┐┌─────────┐┌─────────┐ ░ ┌─┐         \n",
       "   q_0: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░─┤M├─────────\n",
       "        ├─────────┤├─────────┤├─────────┤ ░ └╥┘┌─┐      \n",
       "   q_1: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░──╫─┤M├──────\n",
       "        ├─────────┤├─────────┤├─────────┤ ░  ║ └╥┘┌─┐   \n",
       "   q_2: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░──╫──╫─┤M├───\n",
       "        ├─────────┤├─────────┤├─────────┤ ░  ║  ║ └╥┘┌─┐\n",
       "   q_3: ┤ Rx(0.4) ├┤ Ry(0.5) ├┤ Rz(0.6) ├─░──╫──╫──╫─┤M├\n",
       "        └─────────┘└─────────┘└─────────┘ ░  ║  ║  ║ └╥┘\n",
       "meas: 4/═════════════════════════════════════╩══╩══╩══╩═\n",
       "                                             0  1  2  3 "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit.visualization import circuit_drawer\n",
    "\n",
    "# Create the quantum circuit\n",
    "circuit = QuantumCircuit(4)\n",
    "\n",
    "# Add gate operations to the circuit for keys\n",
    "for i in range(4):\n",
    "    circuit.rx(0.4, i)\n",
    "    circuit.ry(0.5, i)\n",
    "    circuit.rz(0.6, i)\n",
    "\n",
    "# Measure the keys\n",
    "circuit.measure_all()\n",
    "\n",
    "# Draw the circuit diagram\n",
    "circuit_drawer(circuit, output='text')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('Qistik')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "667df7373ffb018a1a239c470215b88f7a3f7ff7ba7f62fed15530f4f71cc92c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
